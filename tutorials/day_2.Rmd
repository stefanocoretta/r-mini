---
title: 'Tutorial: Day 1'
author: "Dr Stefano Coretta"
date: "2022-11-28"
output: html_document
---

## Reading files

During the first day of the workshop, you worked with data that was made available to you through R packages (like sqmf and glottologR).

**But what if you want to import data files from your own research project?**

Easy! You can use for example the `read_csv()` function to read a .csv (comma separated value) file. But before we dive in, let's first talk about some computer basics.

### Files, folder and file extensions

Files saved on your computer live in a specific place. For example, if you download a file from a browser (like Google Chrome, Safari or Firefox), the file is normally saved in the `Download` folder.

But where does the `Download` folder live? Usually, In your user folder! The user folder normally is the name of your account or a name you picked when you created your computer account. In my case, my user folder is simply called `ste`.

So, let's assume I download a file, let's say `big_data.csv`, in the `Download` folder of my user folder.

Now we can represent the location of the `big_data.csv` file like so:

```
ste/
└── Downloads/
    └── big_data.csv
```

To signal that `ste` and `Downloads` are folders, we add a final forward slash `/`. That simply means "hey! I am a folder!".
`big_data.csv` is a file, so it doesn't have a final `/`.

Instead, the file name `big_data.csv` has a **file extension**. The file extension is `.csv`. A file extension signals the type of file: in this the `big_data` file is a `.csv` file, a comma separated value file (we will see an example of what that looks like later).

Different file type have different file extensions:

- Excel files: `.xlsx`.
- Plain text files: `.txt`.
- Images: `.png`, `.jpg`, `.gif`.
- Audio: `.mp3`, `.wav`.
- Video: `.mp4`, `.mov`, `.avi`.
- Etc...

### File paths

Now, we can use an alternative, more succinct way, to represent the location of the `big_data.csv`:

`ste/Downloads/big_data.csv`

This is called a **file path**! It's the path through folders that lead you to the file. Folders are separated by `/` and the file is marked with the extension `.csv`.

Now the million pound question: *where does `ste/` live on my computer???*

User folders are located in different places depending on the operating system you are using:


- On **macOS**: the user folder is in `/Users/`.

  - You will notice that the is a forward slash also before the name of the folder. That is because the `/Users/` folder is a top folder, i.e. there are no folders further up in the hierarchy of folders.
  - This means that the full path for the `big_data.csv` file on a computer running macOS would be: `/Users/ste/Downloads/big_data.csv`.

- On **Windows**: the user folder is in `C:/Users/`

  - You will notice that `C` is followed by a colon `:`. That is because `C` is a drive, which contains files and folders. `C:` is not contained by any other folder, i.e. there are no other folders above `C:` in the hierarchy of folders.
  - This means that the full path for the `big_data.csv` file on a Windows computer would be: `C:/Users/ste/Downloads/big_data.csv`.


When a file path starts from a top-most folder, we call that the **absolute** file path.

There is another type of file paths, called **relative** paths. For sake of time, we won't look at those. So just make sure you have understood how absolute file paths work.

Importing files in R is very easy with the tidyverse packages. You just need to know the file type (very often the file extension helps) and the location of the file (i.e. the file path).

The next section shows you how to import data in R!

## Import data from files

For this tutorial, we will use the data from this paper: SECOND LANGUAGE USERS EXHIBIT SHALLOW MORPHOLOGICAL PROCESSING <https://doi.org/10.1017/S0272263120000170>.

Here is the full abstract:

> The present study tests the Shallow Structure Hypothesis (SSH), which claims that compared to L1 processing, L2 language processing generally underuses grammatical information, prioritizing nongrammatical information. Specifically, this cross-modal priming study tests SSH at the level of morphology, investigating whether late advanced L2 learners construct hierarchically structured representations for trimorphemic derived words during real-time processing as native speakers do. Our results support SSH. In lexical decision on English trimorphemic words (e.g., *unkindness* or [[un-[kind]]-ness]), L1 recognition of the targets was facilitated by their bimorphemic morphological-structural constituent primes (e.g., *unkind*), but not by their bimorphemic nonconstituent primes (e.g., *kindness*), which were only semantically and formally related to the target. In contrast, L2 recognition was equally facilitated by both constituent and nonconstituent primes. These results suggest that unlike L1 processing, L2 processing of multimorphemic words is not mainly guided by detailed morphological structure, overrelying on nonstructural information.

### About the data file

The data is saved in the file `shallow.csv`. This is a `.csv` file. It looks like this (the following is just an excerpt from the file, specifically the first 5 lines):

```
Group,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching
L1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA
L1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left
L1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left
L1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA
```

A `.csv` file is basically a format to save tabular data (i.e. data that looks like a table).
To separate each column, a `.csv` file uses a comma `,` (hence the name "comma separated values").

The first line of the file indicates the names of the columns of the table:

```
Group,ID,List,Target,ACC,RT,logRT,Critical_Filler,Word_Nonword,Relation_type,Branching
```

There are 11 columns.

Then, the rest of the file contains the other rows of the table, with values for each of the 11 columns. Of course, separated by commas.

```
L1,L1_01,A,banoshment,1,423,6.0474,Filler,Nonword,Phonological,NA
L1,L1_01,A,unawareness,1,603,6.4019,Critical,Word,Unrelated,Left
L1,L1_01,A,unholiness,1,739,6.6053,Critical,Word,Constituent,Left
L1,L1_01,A,bictimize,1,510,6.2344,Filler,Nonword,Phonological,NA
```

The `shallow.csv` file should be placed in the `tutorials/` folder of the `r-mini/` folder.

Now, let's import the data!

### Import the data

Importing `.csv` files is very easy. You can use the `read_csv()` function from the tidyverse.

The `read_csv()` function only requires you to specify the file path.

On my computer, the file path of `shallow.csv` is `/Users/ste/r-mini/tutorials/shallow.csv`, but on your computer the file path will be different, of course.

Also, note that it is not enough to use the `read_csv()` function. You also must assign the output of the `read_csv()` function (i.e. the data we are reading) to a variable, using the assignment arrow `<-`.

And since the `read_csv()` is a function from the tidyverse, you first need to attach the tidyverse with `library(tidyverse)` (remember, you need to attach packages **only once** in a single Rmarkdown file).

So, putting all together:

```{r import-shallow}
library(tidyverse)

# CHANGE THE FILE PATH TO MATCH THE PATH ON YOUR COMPUTER
shallow <- read_csv("/Users/ste/r-mini/tutorials/shallow.csv")
```

Fantastic!

### View the data

Now we can view the data.

The easiest way is to click on the name of the data listed in the Environment tab, in the top-right panel of RStudio.

You will see a nicely formatted table, as you would in something like Excel.

Data tables in R (i.e. spreadsheet-like data) are called **data frames** or **tibbles**.

The `shallow` data frame contains 6500 observations and 11 columns (called variables in the Environment tab). The 11 columns are the following

- `Group`: `L1` vs `L2` speakers of English.
- `ID`: Subject unique ID.
- `List`: Word list (A to F).
- `Target`: Target word in the lexical decision trial.
- `ACC`: Lexical decision response accuracy (`0` incorrect response, `1` correct response).
- `RT`: Reaction times of response in milliseconds.
- `logRT`: Logged reaction times.
- `Critical_Filler`: Whether the trial was a `filler` or `critical`.
- `Word_Nonword`: Whether the Target was a real `Word` or a `Nonword`.
- `Relation_type`: The type of relation between prime and target word (`Unrelated`, `NonCostituent`, `Constituent`, `Phonological`).
- `Brancing`: Constituent syntactic branching, `Left` and `Right` (shout out to Charlie Puth).

Great stuff! See how easy it was to import data?

You can easily import other file types as long as they are tabular (i.e. spreadsheet-like), like Excel files.

For Excel files (i.e. files with a `.xls` or `.xlsx` extension), you need to attach the readxl package with `library(readxl)` and use the `read_excel()` function.

## Data transformation

**Data transformation** is a fundamental aspect of data analysis.

After the data you need to use is imported into R, you will have to filter rows, create new columns, or join data frames, among many other transformation operations.

Today, you will learn how to filter, mutate and select data!

### Filter data

You can filter a data frame with the `filter()` function, from the dplyr package (part of the tidyverse).

`filter()` needs two arguments:

- The name of the data frame you want to filter.

- At least one rule to filter the data with.

Let's filter the `shallow` data frame.

The column `Critical_Filler` in `shallow` indicates which trials are the critical (experimental) trials or just fillers (distractors).
Let's filter `shallow` so that it returns only the critical trials (`Critical`).

Couldn't be easier! Run the following code chunk to see the result.

```{r shallow-filt}
filter(
  shallow,                         # The data frame
  Critical_Filler == "Critical"    # The rule
)
```

The code above filters the rows of the `shallow` data frame so that only the rows where `Critical_Filler` equals (`==`) `"Critical"` are returned. The "equals" part is represented with **2 equals signs**! 

The output of the code above also shows the number of **rows** and **columns** in the resulting data frame/tibble. Here it says: `A tibble: 1950 x 11`.

So, how many critical trials are there in the tibble?

- 6500.
- 1950.
- 2000.
- 11.


Now let's assign the output of the filtering operation to a new variable, which we shall call `shallow_filt`.

```{r shallow-filt-2}
shallow_filt <- filter(
  shallow,
  Critical_Filler == "Critical"
)
```

View `shallow_filt` now. You will see that the `Critical_Filler` column now contains only `Critical` trials.

### Logical operators

The two equals sign we used above `==` is called a *logical operator*.
Logical operators are special symbols that allow you to compare things in R.

Above we have compared the column `Critical_Filler` with the string `"Critical"`. In other words, `Critical_Filler == "Critical"` means "give me those observations where `Critical_Filler` is equal to `Critical`.

These are the basic logical operators:

- `A == B`: A is *equal* to B.
- `A != B`: A in *not* equal to B.
- `A > B`: A is *greater* than B.
- `A < B`: A is *smaller* than B.

Let's see how they work in more detail.

Logical operators return a "logical" vector, i.e. a vector that contains values that are either `TRUE` or `FALSE`.

Run the following code.

```{r logic}
10 > 4
2 == 3
```

You see that `10 > 4` returns `TRUE` because 10 is greater `>` than 4. On the other hand, `2 == 3` returns `FALSE`, because 2 is not equal to 3.

Now, try the logical operators yourself by completing the following exercise! Fill in the code in the following chunk so that you get `TRUE` in the first 3 comparison statements and `FALSE` in the last 3.
To complete the exercise you are supposed to substitute the `...` with numbers, strings, or operators so that the statements return `TRUE` or `FALSE`.

```{r logic-ex, eval=FALSE}
# Use numbers or strings.
# These should return `TRUE`
... < 20
"a" == ...
"apple" != ...

# Use operators.
# These should return `FALSE`
"zoo" ... "park"
120 ... 43
0.1 ... 0.002
```

### Multiple filtering

You can filter a data frame based on more than one column.
Simply add more rules, separated by commas.

The following code filters the `shallow` tibble so that the output contains only observations that are `Critical` and that have `RT`s greater than 1000.

```{r multi-filt}
shallow %>%
  filter(
    Critical_Filler == "Critical",
    RT > 1000
  )
```


Which of the following statements is correct?

- There are 10 observations that are critical trials and in which the reaction times are greater than 1000 ms.
- Out of a total of 6500 observations, 616 are critical trials or have reaction times that are greater than 1 s.
- There are 616/6500 trials that are critical and generated reaction times that are greater than 1 s.

Now filter the data so that you get only those trials that are `Critical`, of `Unrelated` relation type, right-branching and have RTs that are lower or equal to 494 ms. Just replace the `...` with code. Remember to separate the individual filtering rules with a comma `,`. 

**HINT**: The less or equal and more or equal operators are `<=` and `>=` respectively.

```{r shallow-filt-3}
shallow %>%
  filter(
    ...
  )
```

Excellent!


## Mutate

What if you want to create a new column based on existing columns?

You can create new columns with the `mutate()` function from dplyr.

### How to create a new column

`mutate()` needs the name of the data frame you want to mutate, the name of the new column and the code to create it.

```{r long-rt}
shallow_filt <- mutate(
    shallow_filt,
    long_RT = RT >= 1000
  )

shallow_filt
```

Woah! A lot of new stuff going on.
Let's unpack it.

This time we have saved the output of `mutate()` back into `shallow_filt` using the assignment symbol `<-`, so that we can use the new column in further code.
In other words, we have overwritten `shallow` with the mutated data frame.

Then we use `mutate()` to create a new column called `long_RT`.

Note that the name of the new column is followed by *one* equal sign `=`, not two `==`.
We are not comparing objects, but creating one.

We are telling `mutate()` to fill the new column with `TRUE` if `RT` **is greater than or equal to** `1000` ms, with `FALSE` if `RT` **is not greater than or equal to** `1000` ms.

To test if `RT` is greater than or equal to `1000` ms, we are using the logical operator `>=`.
We are comparing two objects here.

Easy as that!

### If, else?

Sometimes you will need to create a column based on multiple criteria.

There are several ways to do that, but a very common one is by using "if-else" statements.

If-else statements are exactly what you think they are: **if** X is true, do A; **else** if false, do B.

The following code illustrates how if-else statements work in R.

First, we are creating a numeric vector `RT` with some values in it.

Then, we use the `ifelse()` function to return a string vector that contains `"short"` if the value in `RT` is smaller than 1000 ms, and `"long"` otherwise. Run the code to see.

```{r if-else, exercise=TRUE}
RT <- c(554, 1098, 1000, 245, 2502)

ifelse(RT < 1000, "short", "long")
```

Let's break down `ifelse(RT < 1000, "short", "long")`:

- `ifelse()` is the function.
- `RT < 1000` is the rule: "if RT is less than 1000".
- `"short"` is what is returned if `RT < 1000` is `TRUE`.
- `"long"` is what is returned if `RT < 1000` is `FALSE`.

Note that `>` and `<` mean "greater than" and "less than".
This means that `1000 < 1000` is `FALSE`.

The operators `>=` and `<=` mean "greater than or equal to" and "less than or equal to".
So `1000 <= 1000` returns `TRUE`.

### Create a new column

Now that you have understood how `ifelse()` works, we can use it to create a new column in `shallow_filt`.

Let's create a new column in `shallow_filt` called `accuracy`.

The new column `accuracy` should have `"incorrect"` if the `ACC` value is `0`, else it should be `"correct"`.
Try and fill the `...` with the right code in the following code chunk so that the new `accuracy` column is like that. You need to use the `ifelse()` function

**HINT:** Remember, to check if two things are **equal** you need the `==` operator.

```{r if-else-acc, exercise=TRUE}
shallow <- shallow %>%
  mutate(
    # Replace ... with the ifelse() function 
    accuracy = ...
  )

shallow
```

### Count occurences

We can use the `count()` function to count the number of occurrences for each value of a specific column.
Let's count how many trials are correct, i.e. let's count occurences in the `accuracy` column.

The function `count()` takes the name of tibble and the name of column you want to count values in.

```{r count-corr}
shallow_filt %>%
  count(accuracy)
```

How many correct responses are there in the `shallow_filt` tibble?

Note that you can add multiple column names, separated by commas, to get counts for the combinations of values of each column.

Try to get counts of the combination of `accuracy` and `Group`. Replace `...` with the right code.

**HINT**: In `count()`, include the names of the two columns you want to get counts of, separated by commas.

```{r count-corr-group}
shallow_filt %>%
  count(...)
```

Yay!

## Joining data frames/tibbles

Let's now move on onto data on Mancunian English vowel duration.

The data comes in two files:

- `english-durations.csv` which contains vowel durations and trial info.
- `stimuli.csv` which contains info on the stimuli.

In order to really make sense of the data we need to join these two tables into one.
This is what the *join* operations from the tidyverse are for!

### Import data

First, let's import both data tables individually.
Fill in the code below.

**Note** that the `english-duration.csv` file was generated by Praat which uses by default `--undefined--` for NA values.
To correctly import the file you need to manually specify which strings will be read as `NA`, using the `na` argument of `read_csv()`.
I have added that bit of code for you.

```{r read}
# Replace ... with the file paths. Remember they are strings so should be quoted.
eng_dur <- read_csv(..., na = "--undefined--")
stimuli <- read_csv(...)
```

Now inspect the data frames.
You will see that both data frames have a column called `sentence`.
We can use this column to join the `stimuli` data frame with the `eng_dur` data frame! (It will be clearer what this means after you have made the join below).

You will also have noticed that `eng_dur` has 1800 observations, while `stimuli` has only 120.

```{r dims}
# Returns the number of rows and the number of columns
dim(eng_dur)
dim(stimuli)
```

So what is going on?

Basically, `stimuli` contains information on the stimuli used in the study.
If you check the individual sentence stimuli in `eng_dur` you will see that these are 120.

```{r sents}
length(unique(eng_dur$sentence))
```

What we want to do is to join `stimuli` so that for each occurrence of a sentence stimulus in `eng_dur` we get information on the stimulus added as extra columns taken from `stimuli`.

### Join the data

We can achieve this using `left_join()` (`full_join()` would also work, but to avoid complicating things, let's stick to `left_join()`. If you want to know more, check the documentation here: <https://dplyr.tidyverse.org/reference/mutate-joins.html>. This is just a taster!).

The function `left_join()` takes two data frames and joins them based on shared columns.
Since in our case the only shared column in `sentence`, the function will use that column to join the data frames.

```{r join}
eng_dur_joined <- left_join(eng_dur, stimuli)
```

You see that R alerts you that it's joining by `sentence`.

View new joined data frame!

Now `eng_dur_joined` has columns from both `eng_dur` and `stimuli`!

## That's all!

Great! You have completed this tutorial.

It's been quite an intense workshop, but now you have learned:

- The basics or R (R as a calculator, variables/vectors, functions, arguments).

- The basics of using RStudio to help you use R.

- File paths and file extensions.

- Read data files, like `.csv` and Excel files.

- Install and attach packages to provide you with extra functionality.

- Create a variety of plots which are suitable for most cases.

- Transform data by filtering or mutating it (i.e. create new columns).

- Joining data frames together.

That's a lot for a 3-hour workshop, so very much congratulations to you to have made it this far!!!


Here below I list some further resources you can use to deepen your knowledge of R.

